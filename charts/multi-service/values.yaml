# -- Overrides the clusterName when used in the naming of resources
nameOverride: ""
# -- Overrides the clusterName and nodeGroup when used in the naming of resources. This should only be used when using a single nodeGroup, otherwise you will have name conflicts
fullnameOverride: ""

# -----------
# Deployment
# -----------

# -- Number of replicas
replicaCount: 1
# -- kind of deployment (Deployment or StatefulSet)
kind: Deployment
# -- How many old ReplicaSets to maintain for the Deployment
revisionHistoryLimit: 3

image:
  # -- Docker image repository
  repository: hello-world
  # -- The Kubernetes imagePullPolicy value
  pullPolicy: IfNotPresent
  # -- Overrides the image tag whose default is the chart appVersion.
  tag: latest

# -- Configuration for imagePullSecrets so that you can use a private registry for your image
imagePullSecrets: []

serviceAccount:
  # -- Specifies whether a service account should be created
  create: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# -- Configurable labels applied to all pods
podLabels: {}

# -- Configurable annotations applied to all pods
podAnnotations: {}

# -- Allows you to set the securityContext for the pod
podSecurityContext: {}
  # fsGroup: 2000

# -- Allows you to set the securityContext for the container
securityContext: {}

# -- Enable livenessProbe
livenessProbe: {}
# -- Enable readinessProbe
readinessProbe: {}
# -- Enable startupProbe
startupProbe: {}

# -- Specifies the minimum number of seconds for which a newly created Pod should be ready without any of its containers crashing, for it to be considered available
minReadySeconds: 30

# -- TBD
terminationGracePeriodSeconds: 60

# -- Specifies the strategy used to replace old Pods by new ones
strategy:
  rollingUpdate:
    maxSurge: 25%
    maxUnavailable: 25%
  type: RollingUpdate

ingress:
  # -- Enable Kubernetes Ingress to expose pods
  enabled: false

ingresses: {}
  # - className: "example1"
  #   backend:
  #     service:
  #       name: "example1"
  #       portName: "http"
  #   annotations: {}
  #   hosts:
  #     - host: chart-example1.local
  #       paths:
  #         - path: /
  #           pathType: ImplementationSpecific
  #   tls: []
  # - backend:
  #     service:
  #       name: "example1"
  #       portName: "http"
  #   className: "example2"
  #   annotations: {}
  #   hosts:
  #     - host: chart-example2.local
  #       paths:
  #         - path: /
  #           pathType: ImplementationSpecific
  #   tls: []

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  requests:
    # -- CPU requests for the Deployment
    cpu: 100m
    # -- Memory requests for the Deployment
    memory: 256Mi
  limits: {}
    # -- CPU limits for the Deployment
    # cpu: 500m
    # -- Memory limits for the Deployment
    # memory: 512Mi

# -- Labels of the node(s) where the application pods are allowed to be executed in. Empty means 'any available node'
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
nodeSelector: {}

# -- Similar to the nodeSelector, but slightly different:
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# -- If the application needs to run on tainted nodes, the application needs to have the corresponding tolerations, so kubernetes can schedule to the tainted nodes.
# If the application is required to run on specific nodes that are tainted, configure also nodeSelector.
# https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
tolerations: []
  # - effect: NoExecute
  #   key: example/node-type
  #   operator: Equal
  #   value: applications

# -- Instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster
topologySpreadConstraints: []

env: {}
  # - name: NAME
  #   valueFrom:
  #     secretKeyRef:
  #       name: KEY_NAME
  #       key: SECRET_KEY

extraenv: {}
extraEnv: {}
  # - name: NAME
  #   valueFrom:
  #     secretKeyRef:
  #       name: KEY_NAME
  #       key: SECRET_KEY


# ---------
# Services
# ---------

services:
  - name: "example1"
    type: ClusterIP
    labels: {}
    annotations: {}
    ports:
      port: 80
      targetPort: 1000
      protocol: TCP
      name: http

  - name: "example2"
    type: ClusterIP
    ports:
      port: 90
      targetPort: 9090
      protocol: TCP
      name: http


# ----------------------
# Pod Disruption Budget
# ----------------------

pdb:
  # -- Configure [Pod Disruption Budget]
  enabled: false
  # -- Minimum number / percentage of pods that should remain scheduled
  minAvailable: # 1
  # -- Maximum number / percentage of pods that may be made unavailable
  maxUnavailable: # 1
  # -- Labels to be added to [Pod Disruption Budget]
  labels: {}
  # -- Annotations to be added to [Pod Disruption Budget]
  annotations: {}


# ------------
# Autoscaling
# ------------

autoscaling:
  enabled: false
  # minReplicas: 1
  # maxReplicas: 100
  # targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80


# -----
# Keda
# -----

keda:
  apiVersion: "keda.sh/v1alpha1"
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  fallback:
    failureThreshold: 3
    # replicas: 5
  pollingInterval: 30
  cooldownPeriod: 300
  restoreToOriginalReplicaCount: false
  scaledObject:
    annotations: {}
  triggers: []
    # - type: prometheus
    #   metadata:
    #     serverAddress: https://<prometheus-host>:9090
    #     metricName: http_requests_total
    #     threshold: '100'
    #     query: sum(rate(http_requests_total{deployment="deployment"}[2m]))
  behavior: {}
    # scaleDown:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #     - type: Pods
    #       value: 1
    #       periodSeconds: 180
    #  scaleUp:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #     - type: Pods
    #       value: 2
    #       periodSeconds: 60


# -----------
# Monitoring
# -----------

metrics:
  # -- Enable and configure a Prometheus serviceMonitor for the chart under this key.
  # @default -- See values.yaml
  enabled: false

  serviceMonitors: {}
    # - serviceName: "example1"
    #   labels: {}
    #   scrapeInterval: "15s"
    #   scrapeTimeout: "10s"
    #   metricsPortName: "metrics"
    #   metricsPath: "/metrics"
    #   metricsScheme: "http"
    # -- Prometheus [RelabelConfigs] to apply to samples before scraping
    #   relabelings: []
    # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion
    #   metricRelabelings: []

  podMonitors: {}
    # - name: "example1"
    #   labels: {}
    #   scrapeInterval: "15s"
    #   scrapeTimeout: "10s"
    #   metricsPortName: "metrics"
    #   metricsPath: "/metrics"
    #   metricsScheme: "http"
    # -- Prometheus [RelabelConfigs] to apply to samples before scraping
    #   relabelings: []
    # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion
    #   metricRelabelings: []
    #   selectorLabels: {}

  # -- Enable and configure Prometheus Rules for the chart under this key.
  # @default -- See values.yaml
  prometheusRule:
    enabled: false
    labels: {}

    # -- Configure additional alerting rules for the chart under this key
    alerting:
      rules: []
        # - alert: HighRequestLatency
        #   expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5
        #   for: 10m
        #   labels:
        #     severity: page
        #   annotations:
        #     summary: High request latency

    # -- Configure additional recording rules for the chart under this key
    recording:
      rules: []
        # - record: instance_id:node_cpu:count
        #   expr: count(node_cpu_seconds_total{mode="idle"}) without (cpu,mode)

# --------
# Storage
# --------

volumes: []

volumeMounts: []

volumeClaimTemplates:
  enabled: false
  # -- persistent volume access Modes
  accessModes: ReadWriteOnce
  # -- persistent volume size
  storage: 10Gi
  # -- volume storage Class
  storageClassName: gp3
  volumeMode: Filesystem


# ----------------
# Init Containers
# ----------------

initContainers: []

# ----------------
# CronJobs
# ----------------

cronjob:
  # -- The schedule in cron format
  schedule: "0 0 * * *"
  # -- Specifies how to treat concurrent executions of a Job
  concurrencyPolicy: Forbid
  # -- Optional deadline in seconds for starting the job
  startingDeadlineSeconds:
  # -- This flag tells the controller to suspend subsequent executions
  suspend: 'false'
  # -- The number of successful finished jobs to retain
  successfulJobsHistoryLimit: 3
  # -- The number of failed finished jobs to retain
  failedJobsHistoryLimit: 1

  job:
    # -- Specifies the maximum desired number of pods the job should run at any given time
    parallelism: 1
    # -- Specifies the desired number of successfully finished pods the job should be run with
    completions: 1
    # -- CompletionMode specifies how Pod completions are tracked
    completionMode: NonIndexed
    # -- Specifies the number of retries before marking this job failed
    backoffLimit: 3
    # -- Specifies the duration in seconds relative to the startTime that the job may be continuously active before the system tries to terminate it
    activeDeadlineSeconds: 3600
    # -- ttlSecondsAfterFinished limits the lifetime of a Job that has finished execution (either Complete or Failed)
    ttlSecondsAfterFinished: 86400
    # -- Suspend specifies whether the Job controller should create Pods or not
    suspend: 'false'
    # -- Restart policy for all containers within the pod
    restartPolicy: OnFailure

    # -- Entrypoint array
    # @default -- `[]`
    command: []
      # - /bin/sh
      # - -c
    # -- Arguments to the entrypoint
    # @default -- `[]`
    args: []
      # - date; echo Hello from the Kubernetes cluster

# -- TBD
# @default -- `[]`
slo: []
  # - type: latency
  #   metric: http_request_duration_seconds_bucket{status="200", le="0.25"}
  #   metricTotal: http_request_duration_seconds_count{status="200"}
  # - type: ratio
  #   metric: http_requests{status=~"5.."}
  #   metricTotal: http_requests

# ----------------
# Rollouts
# ----------------

rollouts:
  # -- Specify rollout enablement
  # @default -- false
  enabled: false

  # -- ScaleDown deployment after rollout migration https://argoproj.github.io/argo-rollouts/migrating/#reference-deployment-from-rollout
  # @default -- `false`
  scaleDownDeployment: false

  # -- Minimum number of seconds for which a newly created pod should be ready without any of its container crashing,
  # for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready)
  # @default -- `30`
  minReadySeconds: 30

  # -- The number of old ReplicaSets to retain.
  # @default -- 3
  revisionHistoryLimit: 3

  # -- The rollback window provides a way to fast track deployments to previously deployed versions.
  # @default -- `3`
  rollbackWindow: 3

  canary:
    # -- Specify rollout canary enablement
    # @default -- `false`
    enabled: false

    # -- TBD
    canaryMetadata:
      # -- TBD
      annotations:
        role: canary
      # -- TBD
      labels:
        role: canary

    # -- TBD
    stableMetadata:
      # -- TBD
      annotations:
        role: stable
      # -- TBD
      labels:
        role: stable

    # -- TBD
    # @default -- `1`
    maxUnavailable: 1

    # -- TBD
    # @default -- `"20%"`
    maxSurge: "20%"

    # -- TBD
    # @default -- `false`
    dynamicStableScale: false

    # -- Enable scaleDownDelaySeconds. Ignored if dynamicStableScale=true
    # @default -- `30`
    scaleDownDelaySeconds: 30

    # -- TBD
    abortScaleDownDelaySeconds: 30

    # -- TBD
    # @default -- `1`
    minPodsPerReplicaSet: 1

    # -- TBD
    # @default -- `1`
    scaleDownDelayRevisionLimit: 1

    # -- TBD
    # @default -- `{}`
    analysis: {}

    # -- Specify canary steps
    # @default -- `[]`
    steps: []

    # -- TBD
    # @default -- `{}`
    antiAffinity: {}
      # requiredDuringSchedulingIgnoredDuringExecution: {}
      # preferredDuringSchedulingIgnoredDuringExecution:
      #   weight: 1 # Between 1 - 100

    # -- TBD
    trafficRouting:
      # -- TBD
      nginx:
        # -- TBD
        # @default -- true
        enabled: true
        # -- TBD
        annotationPrefix: null
        # -- Specify additional Ingress Annotation for traffic routing
        # @default -- `{}`
        additionalIngressAnnotations: {}
          # canary-by-header: X-Canary
          # canary-by-header-value: iwantsit

      # -- TBD
      smi:
        # -- TBD
        # @default -- `false`
        enabled: false
        # -- TBD
        rootService: ""
        # -- TBD
        trafficSplitName: ""

  blueGreen:
    # -- Specify rollout blue-green enablement
    # @default -- `false`
    enabled: false

# -----------------
# External Secrets
# -----------------

externalSecrets:
  # -- Specify external secrets enablement
  # @default -- false
  enabled: false

  # -- TBD
  refreshInterval: 1h

  secretStore:
    # -- TBD
    create: false
    # -- TBD
    name: ""
    # -- TBD
    kind: ""
    # -- TBD
    provider: {}

  # -- TBD
  data: []
