# Default values for motive-service.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.



# -- --------------
# Global
# -- --------------



nameOverride: ""
fullnameOverride: ""

# -- How many old ReplicaSets to maintain for the Deployment
# @default -- 3
revisionHistoryLimit: 3

# -- Configuration for imagePullSecrets so that you can use a private registry for your image
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
  automountServiceAccountToken: true

  iam:
    enabled: false
    role:
      awsAccountID: ''
      eksClusterOIDCIssuer: ''
      maxSessionDuration: 3600
    policy: ""
#    policy: |
#      {
#        "Version": "2012-10-17",
#        "Statement": [
#           {
#             "Sid": "",
#             "Effect": "Allow",
#             "Action": [
#               "secretsmanager:ListSecretVersionIds",
#               "secretsmanager:GetSecretValue",
#               "secretsmanager:GetResourcePolicy",
#               "secretsmanager:DescribeSecret"
#             ],
#             "Resource": [
#               "arn:aws:secretsmanager:eu-west-1:{{ .Values.serviceAccount.iam.role.awsAccountID }}:secret:*"
#             ]
#           }
#        ]
#      }



# -- --------------
# Service
# -- --------------


service:
  # -- kind of deployment (Deployment or StatefulSet)
  kind: Deployment

  # -- Annotations to be added to the controller Deployment or DaemonSet
  annotations: {}

  # -- Labels to be added to the service Deployment or DaemonSet and other resources that do not have option to specify labels
  labels: {}

  # -- `terminationGracePeriodSeconds` to avoid killing pods before we are ready
  ## wait up to 1 minute for the drain of connections
  terminationGracePeriodSeconds: 60

  # -- Specifies the minimum number of seconds for which a newly created Pod should be ready without any of its containers crashing, for it to be considered available
  minReadySeconds: 30

  # -- Number of replicas
  replicaCount: 1

  # -- Specifies the strategy used to replace old Pods by new ones
  updateStrategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate

  # -- Configurable annotations applied to all pods
  podAnnotations: {}

  # -- Configurable labels applied to all pods
  podLabels: {}

  # -- Allows you to set the securityContext for the pod
  podSecurityContext: {}
  #  fsGroup: 2000

  # -- See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for notes on enabling and using sysctls
  sysctls: {}
  #  "net.core.somaxconn": "8192"

  # -- Allows you to set the securityContext for the main container
  # See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for notes on enabling and using sysctls
  containerSecurityContext: {}

  # -- Instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster
  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
#    - maxSkew: 1
#      labelSelector:
#        matchLabels:
#          app.kubernetes.io/name: '{{ include "motive-service.name" . }}'
#          app.kubernetes.io/instance: '{{ .Release.Name }}'
#          app.kubernetes.io/component: service
#      topologyKey: topology.kubernetes.io/zone
#      whenUnsatisfiable: ScheduleAnyway
#    - maxSkew: 1
#      labelSelector:
#        matchLabels:
#          app.kubernetes.io/name: '{{ include "motive-service.name" . }}'
#          app.kubernetes.io/instance: '{{ .Release.Name }}'
#          app.kubernetes.io/component: service
#      topologyKey: kubernetes.io/hostname
#      whenUnsatisfiable: ScheduleAnyway

  # -- Labels of the node(s) where the application pods are allowed to be executed in. Empty means 'any available node'
  # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux

  # -- If the application needs to run on tainted nodes, the application needs to have the corresponding tolerations, so kubernetes can schedule to the tainted nodes.
  # If the application is required to run on specific nodes that are tainted, configure also nodeSelector.
  # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
  #  - key: "key"
  #    operator: "Equal|Exists"
  #    value: "value"
  #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Pod anti-affinity can prevent the scheduler from placing service replicas on the same node.
  ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
  ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
  ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
  ##
  podAntiAffinity: "hard"

  ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
  ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
  ##
  podAntiAffinityTopologyKey: kubernetes.io/hostname

  # -- Similar to the nodeSelector, but slightly different:
  # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: kubernetes.io/e2e-az-name
  #         operator: In
  #         values:
  #         - e2e-az1
  #         - e2e-az2

  image:
    # -- Docker image repository
    repository: hello-world
    # -- The Kubernetes imagePullPolicy value
    pullPolicy: IfNotPresent
    # -- Overrides the image tag whose default is the chart appVersion.
    tag: latest

  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    requests:
      # -- CPU requests for the Deployment
      cpu: 100m
      # -- Memory requests for the Deployment
      memory: 256Mi
    limits: {}
      # -- CPU limits for the Deployment
      # cpu: 500m
      # -- Memory limits for the Deployment
      # memory: 512Mi

  # -- Improve connection draining when ingress controller pod is deleted using a lifecycle hook:
  lifecycle:
  #  preStop:
  #    exec:
  #      command:
  #        - /wait-shutdown

  # -- Enable livenessProbe
  livenessProbe: {}
  # -- Enable readinessProbe
  readinessProbe: {}
  # -- Enable startupProbe
  startupProbe: {}

  env: []
    # - name: NAME
    #   valueFrom:
    #     secretKeyRef:
    #       name: KEY_NAME
    #       key: SECRET_KEY

  extraEnv: []
    # - name: NAME
    #   valueFrom:
    #     secretKeyRef:
    #       name: KEY_NAME
    #       key: SECRET_KEY

  envFrom: []
    # - secretRef:
    #     name: secret-name
    # - configMapRef:
    #     name: config-map-name

  type: ClusterIP

  ports:
    servicePort: 8080
    serviceProtocol: TCP
    metricsPort: 8081
    metricsProtocol: TCP


  # -- ------
  # Storage
  # -- ------

  # -- Additional volumes to the controller pod.
  extraVolumes: []
  #  - name: copy-portal-skins
  #    emptyDir: {}

  # -- Additional volumeMounts to the service main container.
  extraVolumeMounts: []
  #  - name: copy-portal-skins
  #   mountPath: /var/lib/lemonldap-ng/portal/skins

  # -- --------------
  # Init Containers
  # -- --------------

  # -- Containers, which are run before the app containers are started.
  extraInitContainers: []
  # - name: init-myservice
  #   image: busybox
  #   command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']

  # -- ----------
  # Autoscaling
  # -- ----------

  # Mutually exclusive with keda autoscaling
  autoscaling:
    enabled: false
    annotations: {}
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    behavior: {}
    # scaleDown:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 1
    #     periodSeconds: 180
    # scaleUp:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 2
    #     periodSeconds: 60

  autoscalingTemplate: []
  # Custom or additional autoscaling metrics
  # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics
  # - type: Pods
  #   pods:
  #     metric:
  #       name: nginx_ingress_controller_nginx_process_requests_total
  #     target:
  #       type: AverageValue
  #       averageValue: 10000m

  # -- ---
  # Keda
  # -- ---

  keda:
    apiVersion: "keda.sh/v1alpha1"
    ## apiVersion changes with keda 1.x vs 2.x
    ## 2.x = keda.sh/v1alpha1
    ## 1.x = keda.k8s.io/v1alpha1
    enabled: false
    minReplicas: 1
    maxReplicas: 5
    fallback:
      failureThreshold: 3
      # replicas: 5
    pollingInterval: 30
    cooldownPeriod: 300
    restoreToOriginalReplicaCount: false
    scaledObject:
      annotations: {}
    triggers: []
      # - type: prometheus
      #   metadata:
      #     serverAddress: https://<prometheus-host>:9090
      #     metricName: http_requests_total
      #     threshold: '100'
      #     query: sum(rate(http_requests_total{deployment="deployment"}[2m]))
    behavior: {}
      # scaleDown:
      #   stabilizationWindowSeconds: 300
      #   policies:
      #     - type: Pods
      #       value: 1
      #       periodSeconds: 180
      #  scaleUp:
      #   stabilizationWindowSeconds: 300
      #   policies:
      #     - type: Pods
      #       value: 2
      #       periodSeconds: 60


  # -- --------------------
  # Pod Disruption Budget
  # -- --------------------

  pdb:
    # -- Minimum number / percentage of pods that should remain scheduled
    minAvailable: 1
    # -- Maximum number / percentage of pods that may be made unavailable. If set, 'minAvailable' is ignored.
    maxUnavailable: # 1
    # -- Labels to be added to [Pod Disruption Budget]
    labels: {}
    # -- Annotations to be added to [Pod Disruption Budget]
    annotations: {}


# -- --------------
# RBAC
# -- --------------

rbac:
  # -- Specifies whether RBAC resources should be created
  enabled: false

  # -- List of roles to create
  roles: []
#    - name: 'test'
#      labels: {}
#      annotations: {}
#      rules:
#        - apiGroups: [""]
#          resources: ["secrets"]
#          verbs: ["get", "watch", "list"]

  # -- List of role bindings to create
  bindings: []
#    - name: 'test-binding'
#      labels: {}
#      annotations: {}
#      roleRef:
#        kind: 'Role' # Role or ClusterRole (default is Role)
#        name: 'test-role-{{ include "motive-service.fullname" $ }}'
#      subjects:
#        - kind: 'ServiceAccount' # User, Group, ServiceAccount
#          name: 'default'
#          namespace: 'default'

# -- --------------
# External Secrets
# -- --------------


externalSecrets:
  secretStores: []
#    - name: "test"
#      kind: "SecretStore|ClusterSecretStore"
#      labels: {}
#      annotations: {}
#      provider:
#        aws:
#          service: "SecretsManager|ParameterStore"
#          region: ""
#          role: ""

  externalSecrets: []
#    - name: "test"
#      labels: {}
#      annotations: {}
#      refreshInterval: 1h
#      secretStoreRefName: "test"
#      secretStoreRefKind: "SecretStore|ClusterSecretStore"
#      targetCreationPolicy: "Owner|Merge|None"
#      targetDeletionPolicy: "Delete|Merge|Retain"
#      targetImmutable: false
#      targetTemplateData:
#        USERNAME: "{{ .username }}
#      data:
#        - secretKey: username
#          remoteRef:
#            key: database-credentials
#            version: v1
#            property: username
#            decodingStrategy: "None|Base64|Base64URL|Auto"
#      dataFrom: {}



# -- --------------
# Ingress
# -- --------------


ingress:
  internal:
    enabled: false
    className: "nginx-internal"
    annotations: {}
      # nginx.ingress.kubernetes.io/enable-opentelemetry: "true"
      # nginx.ingress.kubernetes.io/load-balance: "ewma"
      # nginx.ingress.kubernetes.io/use-regex: "false"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
            portName: http
            port: 8080
    tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local

  public:
    enabled: false
    split: false
    className: nginx-public
    annotations: {}
      # nginx.ingress.kubernetes.io/enable-opentelemetry: "true"
      # nginx.ingress.kubernetes.io/load-balance: "ewma"
      # nginx.ingress.kubernetes.io/use-regex: "false"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
            portName: http
            port: 8080
    tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local



# -- --------------
# Monitoring
# -- --------------


metrics:
  # -- Enable and configure a Prometheus serviceMonitor for the chart under this key.
  # @default -- See values.yaml
  enabled: false

  serviceMonitor:
    enabled: false
    ## The label to use to retrieve the job name from.
    ## jobLabel: "app.kubernetes.io/name"
    namespace: ""
    namespaceSelector: {}
    labels: {}
    annotations: {}
    scrapeInterval: 30s
    scrapeTimeout: 10s
    # honorLabels: true
    targetLabels: []
    # -- Prometheus [RelabelConfigs] to apply to samples before scraping
    relabelings: []
    # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion
    metricRelabelings: []
    metricsPath: "/metrics"
    metricsScheme: "http"

  podMonitor:
    enabled: false
    namespace: ""
    namespaceSelector: {}
    labels: {}
    annotations: {}
    scrapeInterval: 30s
    scrapeTimeout: 10s
    # honorLabels: true
    podTargetLabels: []
    # -- Prometheus [RelabelConfigs] to apply to samples before scraping
    relabelings: []
    # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion
    metricRelabelings: []
    metricsPath: "/metrics"
    metricsScheme: "http"

  # -- Enable and configure Prometheus Rules for the chart under this key.
  # @default -- See values.yaml
  prometheusRule:
    enabled: false
    namespace: ""
    labels: {}
    annotations: {}

    # -- Configure default alerting rules
    defaultAlerts:
      extraLabels: {}
      team: ""
      slackChannel: ""
      slackChannelInfo: ""
      slackChannelWarning: ""
      slackChannelCritical: ""
      kafka:
        enabled: false
        deadLetter:
          enabled: false
          topic:
        lag:
          enabled: false
          namespace: 'kafka-events'
          consumerGroup:
          topics: []
            # - topic: my-topic-1
            #   threshold: 10
            #   for: 10s
            #   consumerGroup: my-consumer-group

    # -- Configure additional alerting rules for the chart under this key
    extraAlertingRules: []
      # - alert: HighRequestLatency
      #   expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5
      #   for: 10m
      #   labels:
      #     severity: critical
      #     team: motive
      #     slack_channel: 'motive-alerts-dev'
      #   annotations:
      #     summary: High request latency

    # -- Configure additional recording rules for the chart under this key
    extraRecordingRules: []
      # - record: instance_id:node_cpu:count
      #   expr: count(node_cpu_seconds_total{mode="idle"}) without (cpu,mode)



# -- --------------
# SLOs
# -- --------------

slos:
  # -- Latency SLOs
  latency: []
    # - name: latency
    #   description: ""
    #   team: ""
    #   slackChannel: ""
    #   extraLabels: {}
    #   latency: 200ms # native latency (or metric)
    #   metric: http_request_duration_seconds_bucket{status="200", le="0.25"}
    #   metricTotal: http_request_duration_seconds_count{status="200"}
    #   groupBy: []
    #   alerting:
    #     name: ErrorBudgetBurn
    #     absent: true
    #     burnrates: true
    #     disabled: false

  # -- Ration SLOs
  ratio: []
    # - name: ratio
    #   description: ""
    #   team: ""
    #   slackChannel: ""
    #   extraLabels: {}
    #   metric: http_requests{status=~"5.."}
    #   metricTotal: http_requests
    #   groupBy: []
    #   alerting:
    #     name: ErrorBudgetBurn
    #     absent: true
    #     burnrates: true
    #     disabled: false

  # -- Bool SLOs
  bool: []
    # - name: bool
    #   description: ""
    #   team: ""
    #   slackChannel: ""
    #   extraLabels: {}
    #   metric: http_requests{status="500"} > 0
    #   target: 0.01
    #   alerting:
    #     name: ErrorBudgetBurn
    #     absent: true
    #     burnrates: true
    #     disabled: false

# -- --------------
# Rollouts
# -- --------------


rollouts:
  # -- Specify rollout enablement
  # @default -- false
  enabled: false

  # -- ScaleDown deployment after rollout migration https://argoproj.github.io/argo-rollouts/migrating/#reference-deployment-from-rollout
  # @default -- `false`
  scaleDownDeployment: false

  # -- Minimum number of seconds for which a newly created pod should be ready without any of its container crashing,
  # for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready)
  # @default -- `30`
  minReadySeconds: 30

  # -- The number of old ReplicaSets to retain.
  # @default -- 3
  revisionHistoryLimit: 3

  # -- The rollback window provides a way to fast track deployments to previously deployed versions.
  # @default -- `3`
  rollbackWindow: 3

  canary:
    # -- Specify rollout canary enablement
    # @default -- `false`
    enabled: false

    # -- TBD
    canaryMetadata:
      # -- TBD
      annotations:
        role: canary
      # -- TBD
      labels:
        role: canary

    # -- TBD
    stableMetadata:
      # -- TBD
      annotations:
        role: stable
      # -- TBD
      labels:
        role: stable

    # -- TBD
    # @default -- `1`
    maxUnavailable: 1

    # -- TBD
    # @default -- `"20%"`
    maxSurge: "20%"

    # -- TBD
    # @default -- `false`
    dynamicStableScale: false

    # -- Enable scaleDownDelaySeconds. Ignored if dynamicStableScale=true
    # @default -- `30`
    scaleDownDelaySeconds: 30

    # -- TBD
    abortScaleDownDelaySeconds: 30

    # -- TBD
    # @default -- `1`
    minPodsPerReplicaSet: 1

    # -- TBD
    # @default -- `1`
    scaleDownDelayRevisionLimit: 1

    # -- TBD
    # @default -- `{}`
    analysis: {}

    # -- Specify canary steps
    # @default -- `[]`
    steps: []

    # -- TBD
    # @default -- `{}`
    antiAffinity: {}
      # requiredDuringSchedulingIgnoredDuringExecution: {}
      # preferredDuringSchedulingIgnoredDuringExecution:
      #   weight: 1 # Between 1 - 100

    # -- TBD
    trafficRouting:
      # -- TBD
      nginx:
        # -- TBD
        # @default -- true
        enabled: true
        # -- TBD
        annotationPrefix: null
        # -- Specify additional Ingress Annotation for traffic routing
        # @default -- `{}`
        additionalIngressAnnotations: {}
          # canary-by-header: X-Canary
          # canary-by-header-value: iwantsit

      # -- TBD
      smi:
        # -- TBD
        # @default -- `false`
        enabled: false
        # -- TBD
        rootService: ""
        # -- TBD
        trafficSplitName: ""
